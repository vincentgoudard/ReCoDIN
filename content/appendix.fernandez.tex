\chapter{Interview : Jose-Miguel Fernandez}
\label{appendix:fernandez}

\section*{Biographie}


\section*{Transcript}

Jose-Miguel Fernandez, interview du 13/06/2018, à l'IRCAM, Paris.

VG — voilà, comme je te disais j'ai une liste de questions assez ouvertes, sur les raisons qui t'ont amené à faire ce que tu fais et là ou tu vas avec les instruments numériques, et les choses qui m'intéressent spécifiquement ce sont les caractéristiques du numérique dans ces instruments qui dès qu'on utilise les instruments sont présentes de toute façon... 

JMF — oui, parce que moi je n'ai pas d'interface physique sauf mes capteurs, mais sinon je fais tout dans l'ordinateur... après bon je fais de la musique mixte donc il y a les instruments vrais qui vont être traités ou de la musique acousmatique pure où il n'y a pas de ...mais je n'ai pas d'interfaces tactiles ou des choses comme ça même si j'en ai fait, j'ai bidouillé pas mal avec arduino, tout ça, mais maintenant je suis plus orienté vers le développement, tous ces trucs là, plus informatique, Antescofo, comme je t'ai montré... 

VG — oui, après tout cela s'inscrit dans un écosystème d'outils que tu utilises... l'idée à la base était motivée par le fait que chacun a sa manière très spécifique de faire de la musique avec des instruments numériques, tout le monde peut programmer à sa sauce donc il y a toujours une part de personnalisation, et pas un truc que tu achètes tout fait... même quand tu achètes Ableton Live, tu vas quand même prendre tes propres chemins... ce qui m'intéressait là-dedans, c'est qu'à chaque fois il y a un côté très singulier dans la manière dont chacun s'approprie ces outils 

JMF — ok 

VG — ma première question c'est qu'est ce qui t'a amené à utiliser des instruments de musique numériques plutôt qu'une guitare, un piano, un instrument qui a déjà une tradition, prêt à l'emploi d'une certaine manière... qu'est ce qui t'a motivé à utiliser de tels instruments ? 

JMF — je viens du Chili et je pense que tout vient de la composition, d'essayer de trouver de nouvelles sonorités, toujours, et surtout des interactions avec des instruments et d'autres types de choses, les gestes, l'utilisation de Kinect (l'interface de Microsoft, NdE) mais au tout début, j'étais au Chili et je savais qu'ici en Europe et aux Etats Unis, on utilisait un logiciel qui s'appelle Max, car j'avais écouté quelques compositions faites ici à l'IRCAM et ailleurs, qui utilisaient ça, donc moi j'étais en quatrième année à l'université, on avait un cours d'électroacoustique et j'avais demandé à mon prof est-ce que tu connais ce logiciel qui s'appelle Max ? Et il me dit oui oui, regarde et il ouvre un tiroir et il y avait Max 2.5, donc avec le gros manuel qu'il y avait à l'époque...  

VG — en papier 

JMF — oui, en papier c'était relié avec une spirale... et donc je l'ai pris, et il m'a dit oui il est ici parce qu'on a gagné un projet donc on a acheté un classic 2, un mac classic 2 (todo vérif) et Max parce que voilà il l'avait étudié en Allemagne, il venait d'arriver, donc il disait oui je sais que ce logiciel, tout le monde dit que c'est un peu l'avenir... déjà à l'époque, c'était en 1994 ... et donc il m'a dit mais moi j'ai aucune idée de comment ça marche donc, tiens, débrouilles toi... donc il m'a donné le truc et moi j'ai commencé à regarder, donc j'ai vu le premier tutorial où il y avait « plus » et après « multiplier », deux boites, donc je me dis bon mais ça... ça ne sert pas à faire de la musique ce truc... et bon après je me suis rendu compte qu'il y avait un métro (objet Max métro, servant de métronome, NdE) donc ah ok, on peut faire les rythmes, et après il y a un random (objet Max, NdE) les trucs de bases quoi... donc je peux faire random, et et après je peux contrôler des trucs externes, des synthétiseurs, des samplers... parce qu'à l'époque c'était que MIDI, c'était la version Opcode MIDI... et donc il y avait des samplers et donc je commençais à enregistrer plein de trucs et après les piloter avec Max et après les synthés, tout ça ... et petit à petit je commençais à rentrer dans le monde de la fabrication, et en temps réel, parce qu'on peut dire que j'ai vraiment commencé avec Max, j'avais fait un peu de Csound aussi... et en même temps... et après je suis allé en Argentine où il y avait le LEIPMS (todo, vérif), c'était un laboratoire d'électroacoustique, le meilleur à l'époque c'était en 1996 ... et c'était le meilleur d'Amérique du Sud, il y avait toutes les semaines des gens, John Chowning, Chadabe,  l'IRCAM, Boulez venaient tout le temps... j'étais pendant six mois et j'ai vu passer toute l'informatique musicale, il y avait Max Mathews, tous les gens très très importants, donc ça bougeait beaucoup, ils avaient beaucoup d'argent à l'époque et il faisaient des concerts toutes les semaines, tous les mercredis, même deux parfois... et donc là j'ai continué à faire de la programmation dans Max ... et voilà je pense qu'à partir de là, je me suis mis complètement dans l'informatique et quand je retournais au Chili, je travaillais avec le gars que je te disais, il avait plein d'ordinateurs, j'ai travaillé avec Nato qui était la première librairie pour faire de la vidéo dans Max et le premier système ambisonique aussi, on avait mis une sphère de haut-parleurs, et ... bon après un certain moment, je suis dit bon là il y a une personne qui peut m'apprendre plus de choses et ici voilà... parce que l'autre personne était plutôt dans la musique électronique pour danser plutôt...et moi je voulais continuer la musique plutôt de recherche et d'exploration ... expérimentale d'une certaine façon.. et bon je me suis dit ici il n'y a plus rien à faire, il faut que je parte un peu plus loin... donc je suis allé à Lyon, au CNSM, j'ai fait la postulation et j'ai été accepté, et là voilà j'ai continué encore à développer Max et tout ça .... et bon l'idée c'était toujours de faire des choses les plus interactives possibles entre les musiciens donc j'ai beaucoup travaillé avec des percussionnistes, même aujourd'hui je continue à le faire, et donc mettre toujours des capteurs, des piezos, des choses pour avoir le plus d'information possible dans l'ordinateur pour la synchronisation, pour les traitements et tout ça et ça m'a amené à commencer à développer de plus en plus de différents types de patchs, soit pour la synthèse, soit pour les traitements... et là c'est arrivé que je suis venu à Paris, faire le cursus de composition de l'IRCAM et là, après le cursus, comme ça arrive à beaucoup de compositeurs, bon je ne suis pas si jeune que ça, mais qu'on a rien... d'un jour à l'autre, j'avais une super bourse pour toute cette année, et d'un jour au lendemain je n'avais rien donc j'ai demandé ici, bon est ce qu'il y a un peu de boulot, et il y a Eric Daubresse qui m'a dit oui, il y a un peu de travail, donc j'ai commencé à travailler avec Emmanuel Nunes, qui  avait une notion de l'électronique très fine, et tout de suite j'ai été amené à faire des patchs assez complexes, pour pouvoir ... donc lui son paradigme, c'était que  chaque note allait avoir un ou plusieurs traitements et chaque note allait se spatialiser dans un ensemble de haut-parleurs, un peu une sphère aussi de haut-parleurs, un enveloppement de haut-parleurs, et donc la première pièce que j'ai faite avec lui, c'était pour ensemble et électronique donc il y avait des couches de superpositions d'instruments qui partaient dans tous les haut-parleurs, donc c'était à l'époque j'utilisais encore des boites de messages, mais il y en avait partout et donc c'était assez, disons grand comme travail, et donc à partir de ça après j'ai continué à travailler à l'IRCAM pendant une dizaine d'années en tant que RIM (réalisateur en informatique musicale, NdE), intermittent du spectacle bien sûr... mais mon idée c'était toujours de voir quelle électronique avoir dans le monde de la musique contemporaine, plus ou moins l'équivalent que la musique instrumentale, c'est à dire que la musique instrumentale a un long parcours historique et aussi des techniques, et des virtuosités... on voit même des concerts pour piano de la musique romantique, il y a un rythmique, harmonique et oui, au niveau de la virtuosité, donc normalement c'est ce qu'on a l'habitude, parce que bon c'est quand même assez jeune je pense l'informatique musicale temps réel, et souvent c'est comme si l'électronique reste toujours un accompagnement ou qui est toujours plus simple, on fait des nappes, ou.... Et donc mon idée c'était pourquoi ne pas arriver au même, même si bon, c'est une idée, c'est peut-être encore une utopie, mais pourquoi ne pas arriver au même niveau de sophistication du monde instrumental dans le monde électronique ... et donc à partir de là, j'ai commencé à faire justement des patchs de plus en plus complexes, mais à un certain moment, plutôt récemment, je me suis rendu compte que j'avais besoin quand même d'un autre paradigme, de trouver une autre façon de faire, qui est d'utiliser par exemple Max dans ce cas, parce que je voulais des choses dynamiquement, très rapidement et avoir une grande superposition, de faire une espèce d'orchestration de l'électronique, au même titre que l'instrumental, et ... aussi avec les travaux que j'avais fait avec Emmanuel Nunes, donc lui m'a d'une certaine façon marqué par cette idée de virtuosité électronique, et d'arriver à un système le plus dynamique possible... et donc j'ai commencé à travailler là avec Antescofo, qui est le suivi de partition, donc là on pouvait avoir vraiment un suivi de partition très très précis et même à un niveau rythmique, même si c'est très rapide l'ordinateur va arriver à suivre, bien sûr si c'est des notes, si c'est des modes de jeu un peu particulier, il faut passer à d'autres systèmes mais au moins le suivi de notes marche très bien et aussi on avait le suivi de tempo donc avec toutes ces données on pouvait déjà faire des choses très très sophistiquées mais il restait quand même la contrainte qu'on a, voilà, sur l'ordinateur, souvent, ou sinon sur des ordinateurs en réseau, mais que la CPU monte assez rapidement, dès qu'on commence à utiliser par exemple des phase-vocoder, SuperVP on appelle ça (un objet Max implémentant un algorithme de vocodeur de phase en temps réel, développé à l'IRCAM, NdE) pour faire du time-stretch en temps-réel par exemple ou un traitement FFT, etc. donc on s'aperçoit que dès que tu commence à faire des choses un peu plus dynamiques et rapidement la CPU part et même Max commence à avoir des problèmes au niveau du timing, de la précision du temps... et donc petit à petit j'ai commencé à migrer vers SuperCollider que j'ai au début utilisé comme un synthétiseur, parce que j'étais assez choqué la première fois que j'ai chargé SuperCollider, bon j'ai commencé avec la version 2, mais qu'il y a une seule ligne de code et que ça envoyait tout de suite un son hyper-complexe et donc je me suis dit bon, là il y a quelque chose à explorer, parce qu'avec une seule ligne de code très réduite, on arrive à faire des sons assez complexes avec des rythmes et surtout la qualité du son, ça m'a beaucoup choqué parce qu'on avait plutôt l'habitude de Max/MSP, à l'époque ça venait de sortir aussi, c'était en 1998, quelque chose comme ça, 1999, je ne me souviens plus... et donc le son de SuperCollider quand même il y avait une richesse qui était assez particulière, qu'on n'avait pas dans Max/MSP à l'époque, peut-être que maintenant c'est différent, peut-être que maintenant, c'est aussi un paramètre un peu abstrait, mais c'est un paramètre qui est quand même perceptible et que pas mal de gens ont quand même... c'était même une discussion dans les forums de logiciels spécialisé, quel est le logiciel qui sonne le mieux et donc toujours il y avait une tendance vers SuperCollider quand on faisait la relation entre des environnements temps-réel ... et donc comme je t'ai dit, j'ai commencé à utiliser SuperCollider que pour faire des sons, parce que ça m'intéressait la partie synthétique et tous les UGens, qui sont comme les objets dans Max, les objets qui génèrent du traitement du signal, du son, des enveloppes et tout ça, et donc il y avait et il y a toujours une grande richesse de différents types de modules pour faire différents types de choses, stochastiques, randomiques et déterministes, etc. et à un certain moment, j'ai commencé à me dire ah mais pourquoi je fais pas les sons, au lieu de les faire en... parce que je les faisais dans SC et je les enregistrais pour les utiliser comme des bandes pour déclencher après, je me suis dit pourquoi ne pas les faire en temps réel et c'est là que je me suis aperçu qu'il n'y avait pas que la synthèse mais qu'il y avait tout un mécanisme de gestion du temps réel dans SC qui était complètement dynamique et vraiment, James McCartney, la personne qui l'avait créé avait pensé du début à toute cette organisation... donc le logiciel est pensé du début, bon comme Max aussi bien sûr pour faire du temps réel, mais ici pour faire du temps réel dynamiquement... c'est à dire qu'on va pouvoir créer des instances de synthèse, les détruire, les faire évoluer dans le temps, créer plusieurs superpositions et avec une consommation de CPU assez réduite... et donc ça, ça m'attirait beaucoup l'attention ... et bon après je fais le mix, j'ai commencé à faire ça il y a deux ou trois ans... entre les logiciels, parce qu'Antescofo bien sûr a évolué, la première pièce où je l'ai utilisé ça devait être en 2010 ou 2011 et aujourd'hui ce n'est plus qu'un suivi de partition mais aussi un langage de programmation en entier, donc un langage où on va pouvoir gérer sur tout le temps, il y a différentes façon de gérer le temps... comme je t'ai montré il y a le temps absolu, le temps relatif, il y a différents types de courbes, des multi-courbes, multi-dimensionnelles,etc.  Donc le fait que c'est un langage de programmation va me permettre de créer des processus temporels ou rythmiques ou musicaux, pour créer des accords ou n'importe quoi mais aussi pour créer de la synthèse et c'est là où je me suis dit bon, ce que je vais faire c'est marier le monde du suivi de partition avec ce langage de programmation c'est un espèce de méta-séquenceur, parce qu'on peut l'utiliser comme séquenceur aussi, on n'est pas obligé de l'utiliser qu'avec le suivi de partition mais tu peux créer tes séquences, des séquences où tu vas générer aussi en même temps des processus, donc qu'est ce que c'est un processus, ça va être par exemple déclencher une séquence qui va être crée avec un algorithme déterminé ou séquence avec des notes que tu vas mettre dans un réservoir... et bon ça c'est vraiment la base, mais ça va être aussi changer la structure d'une séquence où d'une synthèse avec des inputs, donc c'est là où j'ai commencé à utiliser aussi des capteurs, donc je rentre par OSC (Open Sound Control, NdE) direct les capteurs dans Antescofo et lui il va créer les synthèses et tout ça... donc l'idée d'unir les deux mondes, c'est que les deux sont des systèmes dynamiques dans le sens où dans Antescofo je peux créer un petit processus, et ce processus je vais pouvoir l'appeler, mon processus bien sûr va s'appeler « toto » (rires) et je peux l'instancier toutes les fois que je veux... par exemple toto va faire Do, Mi, Sol ... mais je peux le lancer cinquante fois, il va faire cinquante fois Do Mi Sol et même il va y avoir du chevauchement, ce qui veut dire que la polyphonie est déjà intégrée d'office parce que je peux l'instancier autant de fois que je veux... je peux changer ses paramètres, que ça ne soit pas Do Mi Sol, mais Do Ré Mi Fa Sol La Si Do, et donc je peux aussi en temps réel changer la morphologie d'une certaine façon ... la ligne mélodique, si on fait une mélodie, je peux la moduler aussi en temps réel et je peux connecter cette modulation avec le monde extérieur, apr exemple les capteurs ou le clavier ou n'importe quoi, par exemple l'analyse du son, j'ai beaucoup travailler avec l'analyse en temps réel des flux audios avec des descripteurs ou des trucs beaucoup plus simple, comme enveloppe follower, etc. donc tu vas pouvoir moduler toutes ces structures qui sont polyphoniques et ce qui m'intéresse c'est que dans SC c'est plus ou moins l'équivalent parce que je vais pouvoir aussi créer des synthèses, tout ce que je veux, dynamiquement, sans avoir besoin de faire un patch par exemple... Bien sûr avant j'ai programmé tout pour que je puisse faire les interconnexions tout ça, il y a beaucoup de programmation avant, mais au moment de la performance on va pouvoir déclencher autant de choses qu'on veut, à la vitesse qu'on veut, bien sûr il y a des limites, mais ça donne un environnement très flexible et je m'approche de mon idée que je te disais au début que je peux pouvoir manipuler l'électronique de manière aussi souple qu'avec les instruments... bien sûr ça ne va pas être quelqu'un qui va le jouer, sauf si on utilise le suivi de partition, à ce moment là je peux utiliser les caractéristiques de l'instrumentiste comme les gestes, le son etc. pour piloter et créer dynamiquement des synthèses, des spatialisations et tout ça mais je veux aussi pouvoir créer des processus qui vont pouvoir créer plusieurs couches de synthèse, donc c'est ce que je suis en train de faire dans lequel il y a le système ambisonique et je veux, voilà, créer des masses sonores qui vont d'un endroit à l'autre, après peut-être aussi avoir une écoute, que je puisse me balader dans la pièce elle-même, la pièce électronique je veux dire, mais qui est un espace virtuel et créer des masses qui vont évoluer, un peu à la Xénakis d'une certaine façon, utiliser des masses sonores et créer par des synthèses granulaires ou différents types de synthèses que je peux envelopper, envoyer dans différentes parties de l'espace en 3D et voilà... mon idée c'est justement de pouvoir donner à l'électronique une vie que normalement on n'a pas l'habitude de le faire, on ne peut pas le faire par exemple avec ProTools, si je me mets à couper des petits échantillons et à faire des processus, peut-être que je vais passer un an à faire une pièce de cinq minutes... donc c'est vraiment pas pratique de faire comme ça et c'est pas adapté parce que je vais avoir des superpositions de, je sais pas, 200 trucs en même temps.. et par exemple gérer dans proTools 200 tracks c'est un peu compliqué et après si tu veux faire du grouping, bien sûr on peut mais c'est pas le but et ce n'est pas des instruments qui sont adaptés pour faire ce genre de choses et c'est pour ça que j'ai eu cette tendance d'aller vers cette richesse de l'électronique qu'on puisse voir comme un espèce d'orchestre électronique mais aussi comme une possibilité d'intégrer des choses de modèles extérieurs comme des modèles mathématiques ou stochastiques ou des... je sais pas , des orbites, pour l'instant je suis en train de faire un catalogue des librairies dans lesquelles je suis en train d'injecter ou programmer différents modules pour différents types de mouvements dans l'espace ... des mouvements rythmiques etc. et après biensûr des enchainements de synthèse et voilà... c'est ça l'idée, c'est de faire une électronique qui soit très très souple et virtuose... ou pas...  

VG — (todo dummy) 

JMF — parce qu'on peut faire des nappes très lentes mais lesquelles il y a beaucoup de superpositions, c'est pas que des choses très articulées rythmiques mais les deux parce que bon la musique que je fais, il y a souvent des parties qui sont un peu plus statiques, mais après ça rentre dans un chaos où ça aprt dans  tous les sens, où peut-être ça peut revenir, donc ça passe d'un monde à l'autre... donc l'idée c'est pour aller dans cette densité, de pouvoir utiliser qui soient un peu plus performants que ceux qu'on a l'habitude d'utiliser... et voilà c'est plutôt où je vais... 

VG — oui... en faisant un grand saut jusqu'au début de ce que tu disais, tu as commencé par la composition instrumentale classique, hors électronique ? 
