\chapter{Interview : Jose-Miguel Fernandez}
\label{appendix:fernandez}

\section*{Biographie}


\section*{Transcript}

Jose-Miguel Fernandez, interview du 13/06/2018, à l'IRCAM, Paris.

VG — voilà, comme je te disais j'ai une liste de questions assez ouvertes, sur les raisons qui t'ont amené à faire ce que tu fais et là ou tu vas avec les instruments numériques, et les choses qui m'intéressent spécifiquement ce sont les caractéristiques du numérique dans ces instruments qui dès qu'on utilise les instruments sont présentes de toute façon... 

JMF — oui, parce que moi je n'ai pas d'interface physique sauf mes capteurs, mais sinon je fais tout dans l'ordinateur... après bon je fais de la musique mixte donc il y a les instruments vrais qui vont être traités ou de la musique acousmatique pure où il n'y a pas de ...mais je n'ai pas d'interfaces tactiles ou des choses comme ça même si j'en ai fait, j'ai bidouillé pas mal avec arduino, tout ça, mais maintenant je suis plus orienté vers le développement, tous ces trucs là, plus informatique, Antescofo, comme je t'ai montré... 

VG — oui, après tout cela s'inscrit dans un écosystème d'outils que tu utilises... l'idée à la base était motivée par le fait que chacun a sa manière très spécifique de faire de la musique avec des instruments numériques, tout le monde peut programmer à sa sauce donc il y a toujours une part de personnalisation, et pas un truc que tu achètes tout fait... même quand tu achètes Ableton Live, tu vas quand même prendre tes propres chemins... ce qui m'intéressait là-dedans, c'est qu'à chaque fois il y a un côté très singulier dans la manière dont chacun s'approprie ces outils 

JMF — ok 

VG — ma première question c'est qu'est ce qui t'a amené à utiliser des instruments de musique numériques plutôt qu'une guitare, un piano, un instrument qui a déjà une tradition, prêt à l'emploi d'une certaine manière... qu'est ce qui t'a motivé à utiliser de tels instruments ? 

JMF — je viens du Chili et je pense que tout vient de la composition, d'essayer de trouver de nouvelles sonorités, toujours, et surtout des interactions avec des instruments et d'autres types de choses, les gestes, l'utilisation de Kinect (l'interface de Microsoft, NdE) mais au tout début, j'étais au Chili et je savais qu'ici en Europe et aux Etats Unis, on utilisait un logiciel qui s'appelle Max, car j'avais écouté quelques compositions faites ici à l'IRCAM et ailleurs, qui utilisaient ça, donc moi j'étais en quatrième année à l'université, on avait un cours d'électroacoustique et j'avais demandé à mon prof est-ce que tu connais ce logiciel qui s'appelle Max ? Et il me dit oui oui, regarde et il ouvre un tiroir et il y avait Max 2.5, donc avec le gros manuel qu'il y avait à l'époque... 

VG — en papier 

JMF — oui, en papier c'était relié avec une spirale... et donc je l'ai pris, et il m'a dit oui il est ici parce qu'on a gagné un projet donc on a acheté un classic 2, un mac classic 2 (todo vérif) et Max parce que voilà il l'avait étudié en Allemagne, il venait d'arriver, donc il disait oui je sais que ce logiciel, tout le monde dit que c'est un peu l'avenir... déjà à l'époque, c'était en 1994 ... et donc il m'a dit mais moi j'ai aucune idée de comment ça marche donc, tiens, débrouilles toi... donc il m'a donné le truc et moi j'ai commencé à regarder, donc j'ai vu le premier tutorial où il y avait « plus » et après « multiplier », deux boites, donc je me dis bon mais ça... ça ne sert pas à faire de la musique ce truc... et bon après je me suis rendu compte qu'il y avait un métro (objet Max métro, servant de métronome, NdE) donc ah ok, on peut faire les rythmes, et après il y a un random (objet Max, NdE) les trucs de bases quoi... donc je peux faire random, et et après je peux contrôler des trucs externes, des synthétiseurs, des samplers... parce qu'à l'époque c'était que MIDI, c'était la version Opcode MIDI... et donc il y avait des samplers et donc je commençais à enregistrer plein de trucs et après les piloter avec Max et après les synthés, tout ça ... et petit à petit je commençais à rentrer dans le monde de la fabrication, et en temps réel, parce qu'on peut dire que j'ai vraiment commencé avec Max, j'avais fait un peu de Csound aussi... et en même temps... et après je suis allé en Argentine où il y avait le LEIPMS (todo, vérif), c'était un laboratoire d'électroacoustique, le meilleur à l'époque c'était en 1996 ... et c'était le meilleur d'Amérique du Sud, il y avait toutes les semaines des gens, John Chowning, Chadabe, l'IRCAM, Boulez venaient tout le temps... j'étais pendant six mois et j'ai vu passer toute l'informatique musicale, il y avait Max Mathews, tous les gens très très importants, donc ça bougeait beaucoup, ils avaient beaucoup d'argent à l'époque et il faisaient des concerts toutes les semaines, tous les mercredis, même deux parfois... et donc là j'ai continué à faire de la programmation dans Max ... et voilà je pense qu'à partir de là, je me suis mis complètement dans l'informatique et quand je retournais au Chili, je travaillais avec le gars que je te disais, il avait plein d'ordinateurs, j'ai travaillé avec Nato qui était la première librairie pour faire de la vidéo dans Max et le premier système ambisonique aussi, on avait mis une sphère de haut-parleurs, et ... bon après un certain moment, je suis dit bon là il y a une personne qui peut m'apprendre plus de choses et ici voilà... parce que l'autre personne était plutôt dans la musique électronique pour danser plutôt...et moi je voulais continuer la musique plutôt de recherche et d'exploration ... expérimentale d'une certaine façon... et bon je me suis dit ici il n'y a plus rien à faire, il faut que je parte un peu plus loin... donc je suis allé à Lyon, au CNSM, j'ai fait la postulation et j'ai été accepté, et là voilà j'ai continué encore à développer Max et tout ça ... et bon l'idée c'était toujours de faire des choses les plus interactives possibles entre les musiciens donc j'ai beaucoup travaillé avec des percussionnistes, même aujourd'hui je continue à le faire, et donc mettre toujours des capteurs, des piezos, des choses pour avoir le plus d'information possible dans l'ordinateur pour la synchronisation, pour les traitements et tout ça et ça m'a amené à commencer à développer de plus en plus de différents types de patchs, soit pour la synthèse, soit pour les traitements... et là c'est arrivé que je suis venu à Paris, faire le cursus de composition de l'IRCAM et là, après le cursus, comme ça arrive à beaucoup de compositeurs, bon je ne suis pas si jeune que ça, mais qu'on a rien... d'un jour à l'autre, j'avais une super bourse pour toute cette année, et d'un jour au lendemain je n'avais rien donc j'ai demandé ici, bon est ce qu'il y a un peu de boulot, et il y a Eric Daubresse qui m'a dit oui, il y a un peu de travail, donc j'ai commencé à travailler avec Emmanuel Nunes, qui avait une notion de l'électronique très fine, et tout de suite j'ai été amené à faire des patchs assez complexes, pour pouvoir ... donc lui son paradigme, c'était que chaque note allait avoir un ou plusieurs traitements et chaque note allait se spatialiser dans un ensemble de haut-parleurs, un peu une sphère aussi de haut-parleurs, un enveloppement de haut-parleurs, et donc la première pièce que j'ai faite avec lui, c'était pour ensemble et électronique donc il y avait des couches de superpositions d'instruments qui partaient dans tous les haut-parleurs, donc c'était à l'époque j'utilisais encore des boites de messages, mais il y en avait partout et donc c'était assez, disons grand comme travail, et donc à partir de ça après j'ai continué à travailler à l'IRCAM pendant une dizaine d'années en tant que RIM (réalisateur en informatique musicale, NdE), intermittent du spectacle bien sûr... mais mon idée c'était toujours de voir quelle électronique avoir dans le monde de la musique contemporaine, plus ou moins l'équivalent que la musique instrumentale, c'est à dire que la musique instrumentale a un long parcours historique et aussi des techniques, et des virtuosités... on voit même des concerts pour piano de la musique romantique, il y a un rythmique, harmonique et oui, au niveau de la virtuosité, donc normalement c'est ce qu'on a l'habitude, parce que bon c'est quand même assez jeune je pense l'informatique musicale temps réel, et souvent c'est comme si l'électronique reste toujours un accompagnement ou qui est toujours plus simple, on fait des nappes, ou... Et donc mon idée c'était pourquoi ne pas arriver au même, même si bon, c'est une idée, c'est peut-être encore une utopie, mais pourquoi ne pas arriver au même niveau de sophistication du monde instrumental dans le monde électronique ... et donc à partir de là, j'ai commencé à faire justement des patchs de plus en plus complexes, mais à un certain moment, plutôt récemment, je me suis rendu compte que j'avais besoin quand même d'un autre paradigme, de trouver une autre façon de faire, qui est d'utiliser par exemple Max dans ce cas, parce que je voulais des choses dynamiquement, très rapidement et avoir une grande superposition, de faire une espèce d'orchestration de l'électronique, au même titre que l'instrumental, et ... aussi avec les travaux que j'avais fait avec Emmanuel Nunes, donc lui m'a d'une certaine façon marqué par cette idée de virtuosité électronique, et d'arriver à un système le plus dynamique possible... et donc j'ai commencé à travailler là avec Antescofo, qui est le suivi de partition, donc là on pouvait avoir vraiment un suivi de partition très très précis et même à un niveau rythmique, même si c'est très rapide l'ordinateur va arriver à suivre, bien sûr si c'est des notes, si c'est des modes de jeu un peu particulier, il faut passer à d'autres systèmes mais au moins le suivi de notes marche très bien et aussi on avait le suivi de tempo donc avec toutes ces données on pouvait déjà faire des choses très très sophistiquées mais il restait quand même la contrainte qu'on a, voilà, sur l'ordinateur, souvent, ou sinon sur des ordinateurs en réseau, mais que la CPU monte assez rapidement, dès qu'on commence à utiliser par exemple des phase-vocoder, SuperVP on appelle ça (un objet Max implémentant un algorithme de vocodeur de phase en temps réel, développé à l'IRCAM, NdE) pour faire du time-stretch en temps-réel par exemple ou un traitement FFT, etc. donc on s'aperçoit que dès que tu commence à faire des choses un peu plus dynamiques et rapidement la CPU part et même Max commence à avoir des problèmes au niveau du timing, de la précision du temps... et donc petit à petit j'ai commencé à migrer vers SuperCollider que j'ai au début utilisé comme un synthétiseur, parce que j'étais assez choqué la première fois que j'ai chargé SuperCollider, bon j'ai commencé avec la version 2, mais qu'il y a une seule ligne de code et que ça envoyait tout de suite un son hyper-complexe et donc je me suis dit bon, là il y a quelque chose à explorer, parce qu'avec une seule ligne de code très réduite, on arrive à faire des sons assez complexes avec des rythmes et surtout la qualité du son, ça m'a beaucoup choqué parce qu'on avait plutôt l'habitude de Max/MSP, à l'époque ça venait de sortir aussi, c'était en 1998, quelque chose comme ça, 1999, je ne me souviens plus... et donc le son de SuperCollider quand même il y avait une richesse qui était assez particulière, qu'on n'avait pas dans Max/MSP à l'époque, peut-être que maintenant c'est différent, peut-être que maintenant, c'est aussi un paramètre un peu abstrait, mais c'est un paramètre qui est quand même perceptible et que pas mal de gens ont quand même... c'était même une discussion dans les forums de logiciels spécialisé, quel est le logiciel qui sonne le mieux et donc toujours il y avait une tendance vers SuperCollider quand on faisait la relation entre des environnements temps-réel ... et donc comme je t'ai dit, j'ai commencé à utiliser SuperCollider que pour faire des sons, parce que ça m'intéressait la partie synthétique et tous les UGens, qui sont comme les objets dans Max, les objets qui génèrent du traitement du signal, du son, des enveloppes et tout ça, et donc il y avait et il y a toujours une grande richesse de différents types de modules pour faire différents types de choses, stochastiques, randomiques et déterministes, etc. et à un certain moment, j'ai commencé à me dire ah mais pourquoi je fais pas les sons, au lieu de les faire en... parce que je les faisais dans SC et je les enregistrais pour les utiliser comme des bandes pour déclencher après, je me suis dit pourquoi ne pas les faire en temps réel et c'est là que je me suis aperçu qu'il n'y avait pas que la synthèse mais qu'il y avait tout un mécanisme de gestion du temps réel dans SC qui était complètement dynamique et vraiment, James McCartney, la personne qui l'avait créé avait pensé du début à toute cette organisation... donc le logiciel est pensé du début, bon comme Max aussi bien sûr pour faire du temps réel, mais ici pour faire du temps réel dynamiquement... c'est à dire qu'on va pouvoir créer des instances de synthèse, les détruire, les faire évoluer dans le temps, créer plusieurs superpositions et avec une consommation de CPU assez réduite... et donc ça, ça m'attirait beaucoup l'attention ... et bon après je fais le mix, j'ai commencé à faire ça il y a deux ou trois ans... entre les logiciels, parce qu'Antescofo bien sûr a évolué, la première pièce où je l'ai utilisé ça devait être en 2010 ou 2011 et aujourd'hui ce n'est plus qu'un suivi de partition mais aussi un langage de programmation en entier, donc un langage où on va pouvoir gérer sur tout le temps, il y a différentes façon de gérer le temps... comme je t'ai montré il y a le temps absolu, le temps relatif, il y a différents types de courbes, des multi-courbes, multi-dimensionnelles,etc. Donc le fait que c'est un langage de programmation va me permettre de créer des processus temporels ou rythmiques ou musicaux, pour créer des accords ou n'importe quoi mais aussi pour créer de la synthèse et c'est là où je me suis dit bon, ce que je vais faire c'est marier le monde du suivi de partition avec ce langage de programmation c'est un espèce de méta-séquenceur, parce qu'on peut l'utiliser comme séquenceur aussi, on n'est pas obligé de l'utiliser qu'avec le suivi de partition mais tu peux créer tes séquences, des séquences où tu vas générer aussi en même temps des processus, donc qu'est ce que c'est un processus, ça va être par exemple déclencher une séquence qui va être crée avec un algorithme déterminé ou séquence avec des notes que tu vas mettre dans un réservoir... et bon ça c'est vraiment la base, mais ça va être aussi changer la structure d'une séquence où d'une synthèse avec des inputs, donc c'est là où j'ai commencé à utiliser aussi des capteurs, donc je rentre par OSC (Open Sound Control, NdE) direct les capteurs dans Antescofo et lui il va créer les synthèses et tout ça... donc l'idée d'unir les deux mondes, c'est que les deux sont des systèmes dynamiques dans le sens où dans Antescofo je peux créer un petit processus, et ce processus je vais pouvoir l'appeler, mon processus bien sûr va s'appeler « toto » (rires) et je peux l'instancier toutes les fois que je veux... par exemple toto va faire Do, Mi, Sol ... mais je peux le lancer cinquante fois, il va faire cinquante fois Do Mi Sol et même il va y avoir du chevauchement, ce qui veut dire que la polyphonie est déjà intégrée d'office parce que je peux l'instancier autant de fois que je veux... je peux changer ses paramètres, que ça ne soit pas Do Mi Sol, mais Do Ré Mi Fa Sol La Si Do, et donc je peux aussi en temps réel changer la morphologie d'une certaine façon ... la ligne mélodique, si on fait une mélodie, je peux la moduler aussi en temps réel et je peux connecter cette modulation avec le monde extérieur, apr exemple les capteurs ou le clavier ou n'importe quoi, par exemple l'analyse du son, j'ai beaucoup travailler avec l'analyse en temps réel des flux audios avec des descripteurs ou des trucs beaucoup plus simple, comme enveloppe follower, etc. donc tu vas pouvoir moduler toutes ces structures qui sont polyphoniques et ce qui m'intéresse c'est que dans SC c'est plus ou moins l'équivalent parce que je vais pouvoir aussi créer des synthèses, tout ce que je veux, dynamiquement, sans avoir besoin de faire un patch par exemple... Bien sûr avant j'ai programmé tout pour que je puisse faire les interconnexions tout ça, il y a beaucoup de programmation avant, mais au moment de la performance on va pouvoir déclencher autant de choses qu'on veut, à la vitesse qu'on veut, bien sûr il y a des limites, mais ça donne un environnement très flexible et je m'approche de mon idée que je te disais au début que je peux pouvoir manipuler l'électronique de manière aussi souple qu'avec les instruments... bien sûr ça ne va pas être quelqu'un qui va le jouer, sauf si on utilise le suivi de partition, à ce moment là je peux utiliser les caractéristiques de l'instrumentiste comme les gestes, le son etc. pour piloter et créer dynamiquement des synthèses, des spatialisations et tout ça mais je veux aussi pouvoir créer des processus qui vont pouvoir créer plusieurs couches de synthèse, donc c'est ce que je suis en train de faire dans lequel il y a le système ambisonique et je veux, voilà, créer des masses sonores qui vont d'un endroit à l'autre, après peut-être aussi avoir une écoute, que je puisse me balader dans la pièce elle-même, la pièce électronique je veux dire, mais qui est un espace virtuel et créer des masses qui vont évoluer, un peu à la Xénakis d'une certaine façon, utiliser des masses sonores et créer par des synthèses granulaires ou différents types de synthèses que je peux envelopper, envoyer dans différentes parties de l'espace en 3D et voilà... mon idée c'est justement de pouvoir donner à l'électronique une vie que normalement on n'a pas l'habitude de le faire, on ne peut pas le faire par exemple avec ProTools, si je me mets à couper des petits échantillons et à faire des processus, peut-être que je vais passer un an à faire une pièce de cinq minutes... donc c'est vraiment pas pratique de faire comme ça et c'est pas adapté parce que je vais avoir des superpositions de, je sais pas, 200 trucs en même temps... et par exemple gérer dans proTools 200 tracks c'est un peu compliqué et après si tu veux faire du grouping, bien sûr on peut mais c'est pas le but et ce n'est pas des instruments qui sont adaptés pour faire ce genre de choses et c'est pour ça que j'ai eu cette tendance d'aller vers cette richesse de l'électronique qu'on puisse voir comme un espèce d'orchestre électronique mais aussi comme une possibilité d'intégrer des choses de modèles extérieurs comme des modèles mathématiques ou stochastiques ou des... je sais pas , des orbites, pour l'instant je suis en train de faire un catalogue des librairies dans lesquelles je suis en train d'injecter ou programmer différents modules pour différents types de mouvements dans l'espace ... des mouvements rythmiques etc. et après biensûr des enchainements de synthèse et voilà... c'est ça l'idée, c'est de faire une électronique qui soit très très souple et virtuose... ou pas... parce qu'on peut faire des nappes très lentes mais lesquelles il y a beaucoup de superpositions, c'est pas que des choses très articulées rythmiques mais les deux parce que bon la musique que je fais, il y a souvent des parties qui sont un peu plus statiques, mais après ça rentre dans un chaos où ça aprt dans tous les sens, où peut-être ça peut revenir, donc ça passe d'un monde à l'autre... donc l'idée c'est pour aller dans cette densité, de pouvoir utiliser qui soient un peu plus performants que ceux qu'on a l'habitude d'utiliser... et voilà c'est plutôt où je vais... 

VG — oui... en faisant un grand saut jusqu'au début de ce que tu disais, tu as commencé par la composition instrumentale classique, hors électronique ? 


JMF — oui, disons ma première pièce c'était classique, un sextuor à cordes, mais je pense que ma deuxième composition déjà c'était une pièce mixte... et après j'ai fait une pièce électronique pure, acousmatique et après une pièce instrumentale et je pense que depuis que je suis rentré au CNSM de Lyon, j'ai fait peut-être une seule pièce acoustique seule, un quatuor de saxophone, le reste c'est que des pièces mixtes ou électroniques, donc oui, on peut dire que j'ai tout de suite commencé à faire de la musique mixte avec des ordinateurs, c'était presque en même temps... 

VG — dans ce que j'entends de ce que tu me racontes, tu étais dans un cursus de composition où du coup tu as appris à composer avec les instruments acoustiques et avec la notation classique, j'imagine, déjà étendue par les symboles de la musique contemporaine, et du coup l'électronique, c'était du coup un moyen d'avoir des outils plus souples, plus dynamiques, pour l'écriture ? Il y avait vraiment cette idée de l'écriture musicale ? Je te pose cette question car parmi les gens que j'ai interviewés, certains sont arrivés dans la musique électronique uniquement pour des raisons de son, par exemple... c'était la possibilité de faire des sons qu'on ne pouvait pas faire sans l'électronique, il y avait une souplesse au niveau du son ... quelque part les deux se rejoignent... La souplesse d'écriture des processus participe de la richesse des sons qu'on peut produire, sûrement, mais je pensais à cela par rapport à cet exemple que tu donnais de ProTools où c'est très difficile de faire 200 pistes parce que tu veux déclencher 200 notes en même temps, ces systèmes électroniques, tu dis que tu souhaites un système plus ouvert, plus dynamique, plus vivant... et du coup c'est des outils pour lesquels il devient très difficile de noter cela avec une notation classique, voire impossible ... on serait autant embêté avec une partition papier pour dessiner 200 notes qu'on le serait avec ProTools... 

JMF — oui bien sûr 

VG — donc ce n'est pas tant le fait que ProTools ne soit pas pratique, c'est que ... 

JMF — oui, c'est pas adapté à la notation bien sûr... après pour revenir aussi à ce que tu dis, pour moi, le plus important c'est le son, aussi ... le résultat sonore c'est ce qui est le plus important, après il y a des moyens pour y arriver... donc l'écriture instrumentale tu peux faire quelque chose plus ou moins complexe, mais moi ce qui m'intéresse c'est le son, ce n'est pas l'écriture pour l'écriture elle-même... par exemple je peux créer des systèmes qui vont me donner des résultats d'écriture, bien sûr je peux utiliser de la CAO, de la composition assistée par ordinateur, qui va me générer automatiquement des choses mais c'est pas ça vraiment qui m'intéresse... je l'ai utilisé, et je vais continuer à l'utiliser, c'est comme avoir une espèce de réservoir de sons... toute l'écriture pour moi, c'est quelque chose qui va vers le résultat final qui est le son... par exemple, moi, ça ne m'intéresse pas de créer des relations hyper complexes au niveau rythmique, mélodique, etc. sans avoir un retour sonore... je pense que la musique algorithmique pure, pour moi, n'a pas trop d'intérêt parce que on se concentre beaucoup trop sur quelque chose de théorique et on laisse de côté justement le son... et le son c'est ça qu'on va finalement entendre... c'est ça qui est la musique, ce n'est pas la théorie qui est derrière... bien sûr après on peut créer des super théories qui vont créer aussi des sons très intéressants et qui peuvent donner aussi des compositions magnifiques, mais après voilà ça dépende du compositeur de comment il arrive à avancer... surtout aujourd'hui où on a tellement de possibilités... mais voilà c'est comme à partir, je sais pas, des années 60, il y a dans la musique contemporaine, pas dans les autres musiques où il n'y a pas eu cette division entre la partie théorique et le son, mais dans la musique contemporaine au moins européenne et occidentale, on peut dire qu'à partir des années 1950, il y a eu un retour vers le son et je suppose qu'on est encore là-dedans parce que finalement on s'est rendu compte que si on veut pas ennuyer un public au bout de cinq minutes d'entendre quelque chose qui finalement change, par exemple c'est ce qu'on appelle la musique structuraliste de années 1950, où tous les paramètres étaient calculées par des combinaisons calculatoires, et au bout de cinq minutes on perd l'attention (la tension?) parce que ça donne toujours la même chose... bon après il y a des gens qui apprécient beaucoup ça, mais en général on a plus je pense la sensation, ou je ne sais pas comment dire, l'intuition de rentrer dans une musique qui peut te prendre et te ramener et te faire voyager, ballader par différents endroits, sonores bien sûr, dans un espace sonore et... voilà c'est ça qui m'intéresse... donc pour moi tous ces outils, qu'ils soient plus ou moins sophistiqués, c'est pour arriver au résultat final qui va être le son et cet agencement de sons qui est finalement la composition, c'est à dire comment je vais pouvoir prendre quelqu'un, ou même moi-même, parce que peut-être que je vais montrer la musique que j'ai écrit à quelqu'un qui va dire c'est n'importe quoi, c'est pas de la musique ... mais au moins pour moi, et j'espère pour quelqu'un d'autre, va pouvoir rentrer dans cet état quand la musique te prend et va te faire comme une montagne russe, et qui soit quelque chose qui ait une émotion ... le plus important c'est la musicalité et c'est le truc un peu magique, car elle a la capacité de te prendre et te faire rentrer dans des espaces mentaux, psychologiques, ou je ne sais pas quoi qu'on ne vit pas dans la réalité normale... quand tu es en train d'écouter attentivement la musique, comme dans une salle de concert, ou quand tu mets un casque, avoir une concentration et te laisser porter par la musique ... pas la musique d'ascenseur ... mais voilà c'est le paradigme du concert qui, bon est aujourd'hui un peu fragilisé car que les gens peuvent sortir de tout ça... mais la musique acousmatique, comme on dit le cinéma pour l'écoute, je suis assez d'accord avec ça, voilà c'est comme quand on va au cinéma, un film dans lequel tu rentres... et le son va réagir dans ta psyché, tes émotions, et toute ta perception...

VG — c'est la part de sensualité, de souvenirs, d'évocations... 

JMF — oui, d'une certaine façon très hédoniste comme pensée... 

VG — ... qui ne sont pas dans les mathématiques 

JMF — oui, pour moi, tout ça c'est des outils... donc les mathématiques, je peux utiliser différents types d'algorithmes chaotiques, stochastiques, etc. mais ça va être seulement des éléments, comme le bruit ou les sinusoïdes, c'est seulement des outils qu'on a la chance aujourd'hui d'avoir toute cette palette qui est des fois mêmetrop énorme, tu peux te perdre parce que tu ne sais pas par où commencer, tellement il y a de possibilités, parce qu'il y a les instruments, il y a les traitements des instruments, tu as le son de synthèse et maintenant en plus tu peux avoir des choses qui peuvent se produire automatiquement de façon très rapide en temps réel, mais... voilà, ces outils si on arrive à se les approprier, et leur donner une directionnalité, une forme musicale ça peut donner je pense des choses très très riches... voilà c'est ça mon idéal ... après si j'arrive à le faire ou pas, bon c'est une autre histoire... 

VG — et tu parlais du fait que tu avais commencé avec Max, puis après SC, as tu l'impression qu'il y a différents instruments numériques — je ne sais pas si tu les appelles instruments d'ailleurs, que tu as développés et utilisés et qui sont identifiés comme entité... est-ce que tu pourrais les compter par exemple ? Est-ce que tu pourrais dire que jusqu'à aujourd'hui tu as fait, une dizaine, une cinquantaine, 200 instruments électroniques ... ou bien est-ce que c'est quelque chose qui était toujours en évolution ? Est-ce qu'il y a des étapes où tu fais un instrument et il a une fin ? 

JMF — non je pense que je n'ai jamais fini un instrument, c'était toujours un \textit{work-in-progress} et je me souviens que... bon je pense que s'il y a quelque chose avec les gens qui font du Max et du SC, la première chose qu'ils essaient de faire — bon, pas tous—, c'est de se créer un environnement ... par exemple, je me souviens dans Max, mon premier environnement, dès qu'est sorti Javascript, ça a été de créer un système de scripting, parce que j'avais l'idée de créer automatiquement les modules et donc j'avais ce que j'appelais un méta-patch, et je l'ai —peut-être pas ici, mais quelque part dans un disque dur, c'est un patch qui disait combien d'entrées tu veux et combien de sorties, est ce que tu veux un spatialisateur, est ce que tu veux un \textit{frequency shifter}, donc j'ai commencé à donner des listes de traitement et après je cliquais un bouton qui s'appelait « build », et il créait tout le patch automatiquement avec les vumètres, les sliders, la matrice audio, la matrice de contrôle avec tous les trucs dessinés, les inputs, les outputs, et donc d'une certaine façon ça c'était un premier instrument, parce que pour moi l'environnement c'est l'instrument et après tu vas seulement lui créer des plugins de cet environnement... et donc c'est un paradigme assez connu, tous les logiciels marchent comme ça, comme Live ou même ProTools, ou Studio Vision pro, ou Cubase déjà avait la notion de plugins et je pense même les mixeurs, on peut dire d'une certaine façon que c'est des plugins qui sont fixes parce que tu as l'équalisateur, le compresseur, etc. mais voilà on peut dire que cet instrument a déjà les plugins incorporés, et mon idée c'était d'avoir un instrument modulaire mais comme je t'ai dit, il n'y a pas que moi, je pense qu'il y a plusieurs gens qui font de l'informatique musicale dans Max, qui se sont créé, j'en ai vu passer surement toi aussi, plusieurs environnement de ce genre, dans lequel tu peux soit créer comme je t'ai dit, une représentation et après le truc va se créer automatiquement, soit que tu vas les faire plus ou moins dynamiquement... donc ça c'était mon premier instrument... et après quand je suis rentré ici à l'ircam, ce que je faisais c'est que j'avais un patch qui avait plein de traitements, donc après je le montrais au compositeur et après je faisais par exemple une réduction ... ou bien je créais en même temps des plugins parce qu'il voulait faire un truc déterminé, une abstraction Max ou un patch Max, que je vais rajouter au patch principal pour générer des processus... Si je veux faire un truc qui fait des rythmes automatiquement, je vais intégrer cette machine dans l'environnement général de Max... et après j'ai beaucoup travaillé avec les capteurs, comment rentrer des données du monde extérieur pour aussi moduler toutes ces machines en tep réel etc. et mon deuxième instrument c'est cet environnement dans SC qui est plus ou moins la même chose, parce que d'un côté j'ai tous mes modules, c'est des plugins, et je pense qu'il n'y a rien de nouveau par rapport à ça... c'est le même concept qu'on a eu de toujours, mais c'est la possibilité que je vais pouvoir construire des environnements tout de suite et dynamiquement et même tout ce que je faisais avant dans le scripting Max, ça prenait des fois quelques minutes, où tu avais le truc qui tournait, la pizza (le sablier sur OSX, NdE), parce qu'il était en train de créer tous les patchs et bpatcher pour la visualisation etc. tandis que là, ça se fait instantanément... Je n'ai pas calculé combien de temps ça prend... Peut-être que si je fais des graphes audio très complexes ça prend quelques milli-secondes mais c'est instantané, on ne le voit pas... et donc c'est la même chose mais augmenté parce que je le fais en temps réel, automatiquement, je fais une description des processus ou des chaînes de traitement, dans une ligne de code et donc c'est ça pour moi maintenant l'avantage de ce deuxième instrument, je peux dire, que j'ai fabriqué c'est le dynamisme... 

